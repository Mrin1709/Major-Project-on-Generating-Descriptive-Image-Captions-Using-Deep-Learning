# Major-Project-on-Generating-Descriptive-Image-Captions-Using-Deep-Learning
ğŸ“¸âœ¨ Image Caption Generator
Deep Learningâ€“Based System for Generating Descriptive Image Captions
ğŸš€ Overview

This project demonstrates a complete deep-learning pipeline that converts images into meaningful human-like captions. By combining InceptionV3 (CNN) for vision and LSTM for language generationâ€”enhanced with attention mechanismsâ€”the system learns to â€œseeâ€ an image and â€œdescribeâ€ it in natural language.

It is designed, trained, and fine-tuned on the MS COCO dataset, achieving strong performance with coherent, context-aware captions.

ğŸ§  Key Features

âœ”ï¸ Encoderâ€“Decoder Architecture (InceptionV3 + LSTM)
âœ”ï¸ Attention Mechanism for focus on key image regions
âœ”ï¸ End-to-End Training with captions + images
âœ”ï¸ Fine-Tuning of CNN for improved visual understanding
âœ”ï¸ BLEU Score Evaluation (Quantitative & Qualitative)
âœ”ï¸ User-Friendly Code Structure (Feature extraction, tokenization, training pipeline)

ğŸ“‚ Dataset â€” MS COCO

330,000+ images

1.5M+ object instances

80 object categories, 91 â€œstuffâ€ categories

5 human-written captions per image

Used for: object detection, segmentation, keypoints, and image captioning

COCO provides rich, diverse scenesâ€”ideal for teaching models to understand real-world images.

ğŸ› ï¸ Workflow Summary
1ï¸âƒ£ Dataset Preparation

Images resized to 299Ã—299

Normalized to 0â€“1

Captions tokenized, encoded, padded

Vocabulary built using Keras Tokenizer

2ï¸âƒ£ Feature Extraction (Encoder â€“ CNN)

A pre-trained InceptionV3 extracts high-level visual features by removing the classification head, outputting compact image representations.

3ï¸âƒ£ Caption Generation (Decoder â€“ LSTM)

An LSTM network takes the image features and generates captions word-by-word, ensuring sentence flow and grammar.

4ï¸âƒ£ Attention Mechanism

Allows the model to â€œlookâ€ at important regions while forming each word.

5ï¸âƒ£ Training & Evaluation

Optimizer: Adam

Loss: Categorical Cross-Entropy

Split: 80% training / 20% validation

Evaluation:

BLEU Score

Human Inspection

6ï¸âƒ£ Fine-Tuning

Unfreezing CNN layers â†’ improves visual detail recognition

Data augmentation: rotation, zoom, flip

Leads to higher BLEU scores and better captions

ğŸ” Results
â­ Strengths

Generates clear, grammatically correct, and relevant captions

Understands relationships (e.g., â€œA cat is sitting on the sofa.â€)

Fine-tuned model performs significantly better

âš ï¸ Challenges

Struggles with abstract or artistic images

Misses subtle interactions (e.g., whispering, emotion cues)

Limited vocabulary for rare concepts not in COCO dataset

ğŸ§¾ Code Structure
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ annotations.json
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ features/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_features.py
â”‚   â”œâ”€â”€ preprocess_captions.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ captions_data.npz
â””â”€â”€ README.md

ğŸ“˜ Technologies Used

TensorFlow / Keras

NumPy, Pandas

InceptionV3 (ImageNet)

LSTM, Attention Mechanism

MS COCO Dataset

ğŸŒŸ Conclusion

This project successfully builds a complete image-captioning system that bridges vision and language. It demonstrates how CNNs and LSTMs can work together to generate meaningful image descriptions.

The model works well for common, natural scenes and sets the stage for more advanced architectures.

ğŸ”® Future Work

ğŸš€ Integrate Vision Transformers (ViT)
ğŸ¤ Use multimodal transformer-based captioning
ğŸ“ˆ Expand dataset for rare or abstract concepts
ğŸ—£ï¸ Add multilingual caption generation

ğŸ™Œ Team

Team-D â€” EvoAstra Major Project
